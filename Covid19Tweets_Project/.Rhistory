iris_filtered <- na.omit(iris)
# Print the dimensions of the dataset after removal
cat("Dimensions after removing missing values: ", dim(iris_filtered), "\n")
# Load the required library
library(ggplot2)
# Count the occurrences of each species
species_counts <- iris %>%
group_by(Species) %>%
summarise(n = n())
# Create the pie chart
ggplot(species_counts, aes(x = "", y = n, label = Species)) +
geom_pie(aes(fill = Species)) +
labs(title = "Distribution of Species in Iris Dataset",
caption = "Source: UCI Machine Learning Repository") +
theme_void() +
theme(
plot.title = element_text(hjust = 0.5),
legend.title = element_text(hjust = 0.5),
legend.position = "bottom"
)
# Create the scatter plot
ggplot(iris, aes(x = Sepal.Length, y = Petal.Length, color = Species)) +
geom_point() +
labs(title = "Sepal Length vs. Petal Length by Species",
x = "Sepal Length (cm)",
y = "Petal Length (cm)") +
theme_classic()
library(ggplot2) library(dplyr)  # Filter for Setosa and plot setosa_data <- filter(iris, Species == 'setosa') ggplot(setosa_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black") +   labs(title = "Setosa Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()  # Filter for Versicolor and plot versicolor_data <- filter(iris, Species == 'versicolor') ggplot(versicolor_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "lightgreen", color = "black") +   labs(title = "Versicolor Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()  # Filter for Virginica and plot virginica_data <- filter(iris, Species == 'virginica') ggplot(virginica_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "salmon", color = "black") +   labs(title = "Virginica Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()
library(ggplot2) library(dplyr)  # Filter for Setosa and plot setosa_data <- filter(iris, Species == 'setosa') ggplot(setosa_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "skyblue", color = "black") +   labs(title = "Setosa Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()  # Filter for Versicolor and plot versicolor_data <- filter(iris, Species == 'versicolor') ggplot(versicolor_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "lightgreen", color = "black") +   labs(title = "Versicolor Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()  # Filter for Virginica and plot virginica_data <- filter(iris, Species == 'virginica') ggplot(virginica_data, aes(x = Sepal.Width)) +   geom_histogram(binwidth = 0.2, fill = "salmon", color = "black") +   labs(title = "Virginica Sepal Width Distribution", x = "Sepal Width", y = "Count") +   theme_minimal()
# Sample data for products sold in 2014 and 2015
products_2014 <- c(30, 25, 40, 35, 20, 45, 50, 55, 60, 40, 30, 25)
products_2015 <- c(35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90)
# Create a data frame
data <- data.frame(Month = month.abb, Year_2014 = products_2014, Year_2015 = products_2015)
# Reshape data for plotting
library(tidyr)
data_long <- gather(data, key = "Year", value = "No_of_Products", -Month)
# Plotting using ggplot2
library(ggplot2)
ggplot(data_long, aes(x = Month, y = No_of_Products, fill = Year)) +
geom_bar(stat = "identity", position = "dodge", color = "black") +
labs(title = "Monthly Products Sold in 2014 and 2015",
x = "Months",
y = "No of Products") +
scale_fill_manual(values = c("Year_2014" = "orange", "Year_2015" = "white")) +
theme_minimal()
# Sample data for products sold in 2014 and 2015
products_2014 <- c(30, 25, 40, 35, 20, 45, 50, 55, 60, 40, 30, 25)
products_2015 <- c(35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90)
# Create a data frame
data <- data.frame(Month = month.abb, Year_2014 = products_2014, Year_2015 = products_2015)
# Reshape data for plotting
library(tidyr)
data_long <- gather(data, key = "Year", value = "No_of_Products", -Month)
# Plotting using ggplot2
library(ggplot2)
ggplot(data_long, aes(x = Month, y = No_of_Products, fill = Year)) +
geom_bar(stat = "identity", position = "dodge", color = "black") +
labs(title = "Monthly Products Sold in 2014 and 2015",
x = "Months",
y = "No of Products") +
scale_fill_manual(values = c("Year_2014" = "orange", "Year_2015" = "green")) +
theme_minimal()
install.packages('arules')
install.packages('arulesViz')
install.packages('RColorBrewer')
library(arules)
library(arulesViz)
library(RColorBrewer)
data("Groceries")
# Load the Groceries dataset
data("Groceries")
str(Groceries)
groceries_df <- as(Groceries, "data.frame")
missing_values <- colSums(is.na(groceries_df))
# Print the number of missing values for each column
print(missing_values)
print(groceries_df)
min_support<-0.006
min_confidence<-0.25
rules<-apriori(Groceries,parameter=list(support=min_support,confidence=min_confidence))
rules
inspect(rules[1:10])
itemFrequencyPlot(Groceries,topN=20,
col=brewer.pal(8,'Pastel2'),
main='Frequent Item Frequency',
type="relative",
ylab="Item Frequency(Relative)")
# Sample data for products sold in 2014 and 2015
products_2014 <- c(30, 25, 40, 35, 20, 45, 50, 55, 60, 40, 30, 25)
products_2015 <- c(35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90)
# Create a data frame
data <- data.frame(Month = month.abb, Year_2014 = products_2014, Year_2015 = products_2015)
# Reshape data for plotting
library(tidyr)
data_long <- gather(data, key = "Year", value = "No_of_Products", -Month)
# Plotting using ggplot2
library(ggplot2)
ggplot(data_long, aes(x = Month, y = No_of_Products, fill = Year)) +
geom_bar(stat = "identity", position = "dodge", color = "black") +
labs(title = "Monthly Products Sold in 2014 and 2015",
x = "Months",
y = "No of Products") +
scale_fill_manual(values = c("Year_2014" = "orange", "Year_2015" = "green")) +
theme_minimal()
install.packages('arules')
install.packages('arulesViz')
install.packages('RColorBrewer')
library(arules)
library(arulesViz)
library(RColorBrewer)
data("Groceries")
# Load the Groceries dataset
data("Groceries")
str(Groceries)
groceries_df <- as(Groceries, "data.frame")
missing_values <- colSums(is.na(groceries_df))
# Print the number of missing values for each column
print(missing_values)
print(groceries_df)
min_support<-0.006
min_confidence<-0.25
rules<-apriori(Groceries,parameter=list(support=min_support,confidence=min_confidence))
rules
inspect(rules[1:10])
itemFrequencyPlot(Groceries,topN=20,
col=brewer.pal(8,'Pastel2'),
main='Frequent Item Frequency',
type="relative",
ylab="Item Frequency(Relative)")
install.packages("RColorBrewer")
install.packages('arules')
install.packages('arulesViz')
install.packages('RColorBrewer')
library(arules)
library(arulesViz)
library(RColorBrewer)
data("Groceries")
min_support<-0.006
min_confidence<-0.25
rules<-apriori(Groceries,parameter=list(support=min_support,confidence=min_confidence))
rules
inspect(rules[1:10])
itemFrequencyPlot(Groceries,topN=20,
col=brewer.pal(8,'Pastel2'),
main='Frequent Item Frequency',
type="relative",
ylab="Item Frequency(Relative)")
install.packages("arulesViz")
install.packages('arules')
install.packages('arulesViz')
install.packages('RColorBrewer')
library(arules)
library(arulesViz)
library(RColorBrewer)
data("Groceries")
min_support<-0.006
min_confidence<-0.25
rules<-apriori(Groceries,parameter=list(support=min_support,confidence=min_confidence))
rules
inspect(rules[1:10])
itemFrequencyPlot(Groceries,topN=20,
col=brewer.pal(8,'Pastel2'),
main='Frequent Item Frequency',
type="relative",
ylab="Item Frequency(Relative)")
install.packages("arules")
install.packages("RColorBrewer")
# Sample data for products sold in 2014 and 2015
products_2014 <- c(30, 25, 40, 35, 20, 45, 50, 55, 60, 40, 30, 25)
products_2015 <- c(35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90)
# Create a data frame
data <- data.frame(Month = month.abb, Year_2014 = products_2014, Year_2015 = products_2015)
# Reshape data for plotting
library(tidyr)
data_long <- gather(data, key = "Year", value = "No_of_Products", -Month)
# Plotting using ggplot2
library(ggplot2)
ggplot(data_long, aes(x = Month, y = No_of_Products, fill = Year)) +
geom_bar(stat = "identity", position = "dodge", color = "black") +
labs(title = "Monthly Products Sold in 2014 and 2015",
x = "Months",
y = "No of Products") +
scale_fill_manual(values = c("Year_2014" = "orange", "Year_2015" = "green")) +
theme_minimal()
# Define the number of products and the cost for each product
products <- 5
costs <- c(10, 20, 30, 40, 50)
# Define the price range for each product
price_range <- 1:1000
# Define the sales function
sales <- function(price) {
return(price * 0.5)
}
# Define the profit function
profit <- function(price) {
return(sales(price) - costs)
}
# Define the Monte Carlo search function
monte_carlo_search <- function() {
prices <- sample(price_range, products, replace=TRUE)
return(sum(profit(prices)))
}
# Run the Monte Carlo search 1000 times
results <- replicate(1000, monte_carlo_search())
# Find the maximum profit
max_profit <- max(results)
# Find the prices that correspond to the maximum profit
optimal_prices <- price_range[which(results == max_profit)[1]]
# Print the results
print(paste("Maximum profit:", max_profit))
print(paste("Optimal prices:", optimal_prices))
# Function to generate initial state
generate_initial_state <- function(dimensions) {
return(runif(dimensions, min = -10, max = 10))  # Assuming range [-10, 10]
}
# Function to calculate the objective value for a state (Example function, can be replaced)
calculate_objective <- function(state) {
return(sum(state^2))
}
# Function to generate a new state by making a small change to the current state
change_function <- function(state, dimensions) {
# Making a small random change to one dimension
index <- sample(1:dimensions, 1)
newState <- state
newState[index] <- newState[index] + runif(1, min = -0.5, max = 0.5)  # Adjust the range of change as needed
return(newState)
}
# Function to select the best state among the current and next states
best_function <- function(current_state, next_state) {
if (calculate_objective(next_state) < calculate_objective(current_state)) {
return(next_state)
} else {
return(current_state)
}
}
# Hill climbing function
hill_climbing <- function(dimensions, iterations) {
# Generate initial state
current_state <- generate_initial_state(dimensions)
# Iterate hill climbing
for (i in 1:iterations) {
# Generate next state
next_state <- change_function(current_state, dimensions)
# Select the better state
current_state <- best_function(current_state, next_state)
}
return(current_state)
}
# Perform hill climbing with 8 dimensions and 10 iterations
best_state <- hill_climbing(dimensions = 8, iterations = 10)
print(best_state)
# Function to generate initial state
generate_initial_state <- function(dimensions) {
return(runif(dimensions, min = -10, max = 10))  # Assuming range [-10, 10]
}
# Function to calculate the objective value for a state (Example function, can be replaced)
calculate_objective <- function(state) {
return(sum(state^2))
}
# Function to generate a set of neighboring solutions
generate_neighbors <- function(current_state, dimensions, num_neighbors) {
neighbors <- matrix(0, nrow = num_neighbors, ncol = dimensions)
for (i in 1:num_neighbors) {
index <- sample(1:dimensions, 1)
neighbors[i, ] <- current_state
neighbors[i, index] <- current_state[index] + runif(1, min = -0.5, max = 0.5)  # Adjust the range of change as needed
}
return(neighbors)
}
# Function to select the best state among the neighboring states, excluding Tabu states
best_neighbor <- function(neighbors, tabu_list) {
best_score <- Inf
best_state <- NULL
for (i in 1:nrow(neighbors)) {
neighbor <- neighbors[i, ]
if (!identical(neighbor, tabu_list)) {
score <- calculate_objective(neighbor)
if (score < best_score) {
best_score <- score
best_state <- neighbor
}
}
}
return(best_state)
}
# Tabu Search function
tabu_search <- function(dimensions, max_iterations, tabu_length, num_neighbors) {
# Generate initial state
current_state <- generate_initial_state(dimensions)
# Initialize Tabu list
tabu_list <- matrix(0, nrow = tabu_length, ncol = dimensions)
# Initialize best solution
best_solution <- current_state
best_score <- calculate_objective(current_state)
# Iterate Tabu Search
for (iteration in 1:max_iterations) {
# Generate neighboring solutions
neighbors <- generate_neighbors(current_state, dimensions, num_neighbors)
# Select the best neighbor
next_state <- best_neighbor(neighbors, tabu_list)
next_score <- calculate_objective(next_state)
# Update Tabu list
tabu_list <- rbind(tabu_list[-1, ], next_state)
# Update current state
current_state <- next_state
# Update best solution if applicable
if (next_score < best_score) {
best_solution <- next_state
best_score <- next_score
}
}
return(best_solution)
}
# Perform Tabu Search with specified parameters
best_solution <- tabu_search(dimensions = 8, max_iterations = 2, tabu_length = 4, num_neighbors = 5)
print(best_solution)
library(shiny); runApp('D:/3rd year 2nd sem/BDA/Project/Code.R')
runApp('D:/3rd year 2nd sem/BDA/Project/Projectcode.R')
# Load necessary libraries
library(forecast)
# Read your time series data into a variable, assuming it's stored in a dataframe called 'data'
# Replace 'your_time_series_column' with the name of the column containing your time series data
ts_data <- ts(data$your_time_series_column)
source("D:/3rd year 2nd sem/BDO/Programs/time_series_using_arima_model.R")
source("D:/3rd year 2nd sem/BDO/Programs/time_series_using_arima_model.R")
source("D:/3rd year 2nd sem/BDO/Programs/time_series_using_arima_model.R")
library(MASS)
even_matrix <- matrix(c(2, 4, 6,
8, 10, 12,
14, 16, 18), nrow = 3, byrow = TRUE)
odd_matrix <- matrix(c(1, 3, 5,
7, 9, 11,
13, 15, 17), nrow = 3, byrow = TRUE)
# Perform matrix operations: addition
addition_result <- even_matrix + odd_matrix
print("Addition Result:")
print(addition_result)
# Perform matrix operations: subtraction
subtraction_result <- even_matrix - odd_matrix
print("Subtraction Result:")
print(subtraction_result)
# Perform matrix operations: multiplication
multiplication_result <- even_matrix %*% odd_matrix
print("Multiplication Result:")
print(multiplication_result)
# Perform matrix transpose
transpose_even_matrix <- t(even_matrix)
transpose_odd_matrix <- t(odd_matrix)
print("Transpose of Even Matrix:")
print(transpose_even_matrix)
print("Transpose of Odd Matrix:")
print(transpose_odd_matrix)
# Check determinant and find inverse for even matrix
even_det <- det(even_matrix)
print("Determinant of Even Matrix:")
print(even_det)
if (even_det != 0) {
even_inverse <- ginv(even_matrix)
print("Inverse of Even Matrix:")
print(even_inverse)
} else {
print("Even Matrix is singular, cannot compute inverse.")
}
# Check determinant and find inverse for odd matrix
odd_det <- det(odd_matrix)
print("Determinant of Odd Matrix:")
print(odd_det)
if (odd_det != 0) {
odd_inverse <- ginv(odd_matrix)
print("Inverse of Odd Matrix:")
print(odd_inverse)
} else {
print("Odd Matrix is singular, cannot compute inverse.")
}
# Load necessary libraries
library(tm)
# Load the dataset
tweets <- read.csv("tweets.csv", stringsAsFactors = FALSE)
source("D:/3rd year 2nd sem/BDA/Endsem/Practical.R")
source("D:/3rd year 2nd sem/BDA/Endsem/Practical.R")
source("D:/3rd year 2nd sem/BDA/Endsem/Practical.R")
# Load necessary libraries
library(tm)
# Load the dataset
tweets <- read.csv("D://3rd year 2nd sem//BDA//Endsem//tweets.csv", stringsAsFactors = FALSE)
# Convert text to lowercase
tweets$text <- tolower(tweets$text)
# Install and load necessary packages
install.packages("tidyverse")
install.packages("textclean")
install.packages("syuzhet")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("stringr")
library(tidyverse)
library(textclean)
library(syuzhet)
library(ggplot2)
library(dplyr)
library(stringr)
# Read the CSV file
tweets <- read_csv("D://3rd year 2nd sem//BDA//Endsem//tweets.csv")
# Preprocess the tweet content
tweets$content <- tweets$content %>%
tolower() %>%
replace_contraction() %>%
remove_punctuation() %>%
remove_numbers() %>%
remove_stopwords()
library(tidyverse)
library(textclean)
library(syuzhet)
library(ggplot2)
library(dplyr)
library(stringr)
# Load sentiment lexicon
data("bing")
# Tokenize the tweets
tweets_tokens <- tweets %>%
unnest_tokens(word, content)
library(tidyverse)
library(textclean)
library(syuzhet)
library(ggplot2)
library(dplyr)
library(stringr)
# Load sentiment lexicon
data("bing")
# Tokenize the tweets
tweets_tokens <- tweets %>%
tidytext::unnest_tokens(word, content)
# Join with sentiment lexicon
tweets_sentiment <- tweets_tokens %>%
inner_join(get_sentiments("bing"))
library(tidyverse)
library(textclean)
library(syuzhet)
library(ggplot2)
library(dplyr)
library(stringr)
# Load sentiment lexicon
data("bing")
# Tokenize the tweets
tweets_tokens <- tweets %>%
tidytext::unnest_tokens(word, content)
# Join with sentiment lexicon
tweets_sentiment <- tweets_tokens %>%
inner_join(get_sentiment("bing"))
library(tm)
# Load the dataset into RStudio
tweets <- read.csv("tweets.csv")
library(tm)
# Load the dataset into RStudio
tweets <- read.csv("D://3rd year 2nd sem//BDA//Endsem//tweets.csv")
# Convert the text to lowercase
tweets$content <- tolower(tweets$content)
# Remove punctuation, numbers, and special characters
tweets$content <- gsub("[^a-zA-Z\\s]", "", tweets$content)
# Tokenize the text into individual words
tweets_tokens <- lapply(tweets$content, function(x) {
unlist(strsplit(x, "\\s+"))
})
# Remove stopwords using the 'tm' package
stopwords <- stopwords("en")  # Load English stopwords
tweets_tokens_nostop <- lapply(tweets_tokens, function(x) {
x[!tolower(x) %in% stopwords]
})
# If you want to join the tokens back into sentences
tweets_clean <- sapply(tweets_tokens_nostop, function(x) {
paste(x, collapse = " ")
})
source("D:/3rd year 2nd sem/BDA/Endsem/Practical.R")
library(tm)
# Load the dataset into RStudio
tweets <- read.csv("tweets.csv")
library(tm)
# Load the dataset into RStudio
tweets <- read.csv("D://3rd year 2nd sem//BDA//Endsem//tweets.csv")
# Display the first few rows of the dataset
head(tweets)
# Convert the text to lowercase
tweets$content <- tolower(tweets$content)
# Display the first few rows of the dataset after converting to lowercase
head(tweets)
# Remove punctuation, numbers, and special characters
tweets$content <- gsub("[^a-zA-Z\\s]", "", tweets$content)
# Display the first few rows of the dataset after removing punctuation, numbers, and special characters
head(tweets)
# Tokenize the text into individual words
tweets_tokens <- lapply(tweets$content, function(x) {
unlist(strsplit(x, "\\s+"))
})
# Display the tokens for the first few tweets
head(tweets_tokens)
# Remove stopwords using the 'tm' package
stopwords <- stopwords("en")  # Load English stopwords
tweets_tokens_nostop <- lapply(tweets_tokens, function(x) {
x[!tolower(x) %in% stopwords]
})
# Display the tokens without stopwords for the first few tweets
head(tweets_tokens_nostop)
# If you want to join the tokens back into sentences
tweets_clean <- sapply(tweets_tokens_nostop, function(x) {
paste(x, collapse = " ")
})
# Display the cleaned tweets for the first few tweets
head(tweets_clean)
library(shiny); runApp('D:/3rd year 2nd sem/BDA/Project/Covid19_Tweets_Sentimental_Analysis.R')
install.packages("htmltools")
library(shiny); runApp('D:/3rd_year_2nd_sem/BDA/BDA_Covid19Tweets_Project/Covid19_Tweets_Sentimental_Analysis.R')
library(shiny); runApp('Covid19_Tweets_Sentimental_Analysis.R')
